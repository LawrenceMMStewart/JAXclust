<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Training a CNN on MNIST with the Partial Fenchel-Young Loss &mdash; JAXClust 0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=2709fde1"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="API reference" href="../api.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            JAXClust
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Training a CNN on MNIST with the Partial Fenchel-Young Loss</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Data-loader-and-Model">Data loader and Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Implementing-Differentiable-Clustering-with-JAXClust">Implementing Differentiable Clustering with JAXClust</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/LawrenceMMStewart/jaxclust/graphs/contributors">Authors</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/LawrenceMMStewart/jaxclust">Source code</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/LawrenceMMStewart/jaxclust/issues">Issue tracker</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">JAXClust</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Training a CNN on MNIST with the Partial Fenchel-Young Loss</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/examples/cnn-mnist.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Training-a-CNN-on-MNIST-with-the-Partial-Fenchel-Young-Loss">
<h1>Training a CNN on MNIST with the Partial Fenchel-Young Loss<a class="headerlink" href="#Training-a-CNN-on-MNIST-with-the-Partial-Fenchel-Young-Loss" title="Link to this heading"></a></h1>
<p>In this example we will train a simple Conv Net on MNIST data using <code class="docutils literal notranslate"><span class="pre">jaxclust</span></code>. We will use:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">jaxclust</span></code>: for differentiable clustering methods.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">flax</span></code>: for our neural network class and train state.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">optax</span></code>: for the optimizer to train our network.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensorflow-datasets</span></code>: to access MNIST.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import sys
sys.path.append(&#39;../..&#39;)

import jax
import numpy as np
import jax.numpy as jnp
from typing import Callable, Tuple, Any
import tensorflow as tf
import jaxclust
import optax
import flax.linen as nn
from flax import core, struct
from flax.training import train_state
import optax
from functools import partial
import tqdm
import matplotlib.cm as cm
import matplotlib.pyplot as plt

np.random.seed(0)
# Ensure TF does not see GPU and grab all GPU memory.
tf.config.set_visible_devices([], device_type=&#39;GPU&#39;)
</pre></div>
</div>
</div>
<section id="Data-loader-and-Model">
<h2>Data loader and Model<a class="headerlink" href="#Data-loader-and-Model" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Using <code class="docutils literal notranslate"><span class="pre">tensorflow-datasets</span></code> we can load the train split of MNIST.</p></li>
<li><p>The function <code class="docutils literal notranslate"><span class="pre">process_nist_batch</span></code> reshapes the data to the shape required for LeNET5, as well as renormalizing the data and creating a <strong>one-hot</strong> representation of the labels.</p></li>
<li><p>The function <code class="docutils literal notranslate"><span class="pre">next_train</span></code> takes an iterator and returns a batch <span class="math notranslate nohighlight">\((x, y)\)</span>, and the iterator (and creates a new iterator if we reach the end of the current one).</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import tensorflow_datasets as tfds
DSHAPE = (28, 28, 1) # shape of an image for CNN
BS = 32
DATA_DIR = &#39;/tmp/tfds&#39;

@jax.jit
def process_nist_batch(x, y):
    x = x.reshape((len(x), ) + DSHAPE)
    x = x / 255.
    yhot = jax.nn.one_hot(y, 10)
    return x, yhot

DS_TRAIN = tfds.load(name=&#39;mnist&#39;, batch_size=-1, data_dir=DATA_DIR, split=&#39;train&#39;, as_supervised=True)
DS_TRAIN = tf.data.Dataset.from_tensor_slices(DS_TRAIN).shuffle(buffer_size=60000, seed=0, reshuffle_each_iteration=True)
DS_TRAIN = DS_TRAIN.batch(batch_size=BS)
train_iterator = iter(tfds.as_numpy(DS_TRAIN))


def next_train(train_iterator):
    try:
        (x, y)= next(train_iterator)
        if x.shape[0] != BS:
            train_iterator = iter(tfds.as_numpy(DS_TRAIN))
            (x, y) = next(train_iterator)
    except StopIteration:
        train_iterator = iter(tfds.as_numpy(DS_TRAIN))
        (x, y)= next(train_iterator)

    x, y = process_nist_batch(x, y)
    return (x, y), train_iterator
<br/><br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2023-10-16 16:45:17.766694: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with &#34;NOT_FOUND: Could not locate the credentials file.&#34;. Retrieving token from GCE failed with &#34;FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning &#39;Couldn&#39;t resolve host name&#39;, error details: Could not resolve host: metadata&#34;.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-bold">Downloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /tmp/tfds/mnist/3.0.1...</span>
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "624da6716acb41e98157540999d6b868", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-bold">Dataset mnist downloaded and prepared to /tmp/tfds/mnist/3.0.1. Subsequent calls will reuse this data.</span>
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
2023-10-16 16:45:21.276731: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
</pre></div></div>
</div>
<p>We begin by creating a simple CNN model in flax (this will be the model used to create our embeddings):</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class CNN(nn.Module):
    &quot;&quot;&quot;A simple CNN model.&quot;&quot;&quot;
    dense1: int = 256 # size of dense layer
    dense2 : int = 256 # size of output layer

    @nn.compact
    def __call__(self, x, training=True):
        x = nn.Conv(features=32, kernel_size=(3, 3))(x)
        x = nn.relu(x)
        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
        x = nn.Conv(features=64, kernel_size=(3, 3))(x)
        x = nn.relu(x)
        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
        x = x.reshape((x.shape[0], -1))  # flatten
        x = nn.Dense(features=self.dense1)(x)
        x = nn.relu(x)
        x = nn.Dense(features=self.dense2)(x)
        return x
<br/></pre></div>
</div>
</div>
</section>
<section id="Implementing-Differentiable-Clustering-with-JAXClust">
<h2>Implementing Differentiable Clustering with JAXClust<a class="headerlink" href="#Implementing-Differentiable-Clustering-with-JAXClust" title="Link to this heading"></a></h2>
<p>Firstly we need to define a similarity measure. There are many choices one could take, but for now lets keep it simple and use the negative Euclidean square distance:</p>
<div class="math notranslate nohighlight">
\[\Sigma_{i,j} = - \|x_i-x_j\|_2^2.\]</div>
<p>To calculate <span class="math notranslate nohighlight">\(\Sigma\)</span> we will write the function <code class="docutils literal notranslate"><span class="pre">pairwise_square_distance</span></code> using jax:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def pairwise_square_distance(X):
    &quot;&quot;&quot;
    euclidean pairwise square distance between data points
    &quot;&quot;&quot;
    n = X.shape[0]
    G = jnp.dot(X, X.T)
    g = jnp.diag(G).reshape(n, 1)
    o = jnp.ones_like(g)
    return jnp.dot(g, o.T) + jnp.dot(o, g.T) - 2 * G
<br/></pre></div>
</div>
</div>
<p>Given a similarity matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> and a number of connected components <span class="math notranslate nohighlight">\(k\)</span>, we recall the maximum weight k-connected-component forest problem:</p>
<div class="math notranslate nohighlight">
\[A_k^*(\Sigma) = \text{argmax}_{A \in \mathcal{C}_k} \langle A, \Sigma \rangle.\]</div>
<p>where <span class="math notranslate nohighlight">\(A_k^*(\Sigma)\)</span> is the adjacency matrix for the maximum weight k-connected-component forest, and <span class="math notranslate nohighlight">\(\mathcal{C}_k\)</span> is the set of adjacency matrices which correspond to k-connected-component forests.</p>
<p>The maxmium weight k-connected-component forest will have weight:</p>
<div class="math notranslate nohighlight">
\[F_k^*(\Sigma) =  \text{max}_{A \in \mathcal{C}_k} \langle A, \Sigma \rangle = \langle A_k^*(\Sigma), \Sigma \rangle.\]</div>
<p>To obtain a solver for this we can use <code class="docutils literal notranslate"><span class="pre">jaxclust.solvers</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># flp = forest linear program
solver = jaxclust.solvers.get_flp_solver(constrained=False, use_prims=True)
</pre></div>
</div>
</div>
<p>Since <code class="docutils literal notranslate"><span class="pre">constrained</span> <span class="pre">=</span> <span class="pre">False</span></code>, the function <code class="docutils literal notranslate"><span class="pre">solver</span></code> will take in two arguments <code class="docutils literal notranslate"><span class="pre">Sigma</span></code> and <code class="docutils literal notranslate"><span class="pre">ncc</span></code> (the number of connected components), and returns <span class="math notranslate nohighlight">\(A^*(\Sigma)\)</span> and <span class="math notranslate nohighlight">\(M^*(\Sigma)\)</span>. Recall that <span class="math notranslate nohighlight">\(M_k^*(\Sigma)_{ij} = 1\)</span> if points <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are in the same connected component / cluster otherwise <span class="math notranslate nohighlight">\(0\)</span>. Let’s try it out on some randomly generated data:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = np.random.randn(BS, 3) # sample a batch of 3D data
S = - pairwise_square_distance(X) # calculate Sigma
A, M = solver(S, 10) # call the solver

fig, axs = plt.subplots(1, 2)
axs[0].imshow(A)
axs[1].imshow(M)
axs[0].set_title(r&#39;$A_{10}(\Sigma)$&#39;)
axs[1].set_title(r&#39;$M_{10}^*(\Sigma)$&#39;)
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Text(0.5, 1.0, &#39;$M_{10}^*(\\Sigma)$&#39;)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_cnn-mnist_14_1.png" src="../_images/examples_cnn-mnist_14_1.png" />
</div>
</div>
<p>Similar using <code class="docutils literal notranslate"><span class="pre">jaxclust.solvers</span></code> we can call the function <code class="docutils literal notranslate"><span class="pre">get_flp_solver</span></code> but with <code class="docutils literal notranslate"><span class="pre">constrained=True</span></code> to obtain a solver that takes in partial cluster coincidence information.</p>
<div class="math notranslate nohighlight">
\[A_k^*(\Sigma, M_\Omega) = \text{argmax}_{A \in \mathcal{C}_k(M_\Omega)} \langle A, \Sigma \rangle\,\]</div>
<p>In the paper <span class="math notranslate nohighlight">\(M_\Omega\)</span> takes values in <span class="math notranslate nohighlight">\(\{0, 1, *\}\)</span>.</p>
<p>For clarity, jaxclust takes the constraints in the form of a matrix <span class="math notranslate nohighlight">\(C\)</span> where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(C_{ij}=1\)</span> implies a <strong>must-link</strong> constraint.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_{ij}=-1\)</span> implies a <strong>must-not-link</strong> constraint.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_{ij}=0\)</span> implies no constraint.</p></li>
</ul>
<p>(Non-important note: if denote <span class="math notranslate nohighlight">\(*=0.5\)</span>, then <span class="math notranslate nohighlight">\(C = 2 * M_\Omega - 1\)</span>).</p>
<p>Once again we can call <code class="docutils literal notranslate"><span class="pre">get_flp_solver</span></code> to obtain a solver, but this time using the <code class="docutils literal notranslate"><span class="pre">constrained=True</span></code> kwarg:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>csolver = jaxclust.solvers.get_flp_solver(constrained=True, use_prims=True)
</pre></div>
</div>
</div>
<p>Now we have a unconstrained solver <code class="docutils literal notranslate"><span class="pre">solver</span></code>, and a constrained solver <code class="docutils literal notranslate"><span class="pre">csolver</span></code>, we can use the <code class="docutils literal notranslate"><span class="pre">jaxclust.perturbations</span></code> module to create their smooth proxies in order to obtain gradients for training.</p>
<p>The perturbed proxies are defined as:</p>
<div class="math notranslate nohighlight">
\[A_{k,\epsilon}^*(\Sigma) = \mathbb{E}_Z\left[\text{argmax}_{A \in \mathcal{C}_k} \langle A, \Sigma + \epsilon Z \rangle\right].\]</div>
<div class="math notranslate nohighlight">
\[A_{k,\epsilon}^*(\Sigma, M_\Omega) =\mathbb{E}_Z\left[ \text{argmax}_{A \in \mathcal{C}_k(M_\Omega)} \langle A, \Sigma + \epsilon Z\rangle\, \right].\]</div>
<div class="math notranslate nohighlight">
\[F_{k,\epsilon}^*(\Sigma) = \mathbb{E}_Z\left[\text{max}_{A \in \mathcal{C}_k} \langle A, \Sigma + \epsilon Z \rangle\right].\]</div>
<div class="math notranslate nohighlight">
\[F_{k,\epsilon}^*(\Sigma, M_\Omega) =\mathbb{E}_Z\left[ \text{max}_{A \in \mathcal{C}_k(M_\Omega)} \langle A, \Sigma + \epsilon Z\rangle\, \right].\]</div>
<p>We can obtain a solver that returns <span class="math notranslate nohighlight">\((A^*_{k,\epsilon} , F^*_{k, \epsilon}, M^*_{k, \epsilon})\)</span> by using the function <code class="docutils literal notranslate"><span class="pre">make_pert_flp_solver</span></code> found in the <code class="docutils literal notranslate"><span class="pre">jaxclust.perturbations</span></code> module, for both constrained and unconstrained solvers:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>NUM_SAMPLES = 100
SIGMA = 0.1

pert_solver = jaxclust.perturbations.make_pert_flp_solver(solver,
                                                          constrained=False,
                                                          num_samples=NUM_SAMPLES,
                                                          sigma=SIGMA)

pert_csolver = jaxclust.perturbations.make_pert_flp_solver(csolver,
                                                           constrained=True,
                                                           num_samples=NUM_SAMPLES,
                                                           sigma=SIGMA)
<br/><br/></pre></div>
</div>
</div>
<p>We can now implement the whole differentiable clustering pipeline as a <code class="docutils literal notranslate"><span class="pre">flax.linen.Module</span></code>.</p>
<p>We define a dataclass called <code class="docutils literal notranslate"><span class="pre">DC</span></code> (differentiable clustering), which takes in:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">backbone</span></code> : This will be the model used to produce the embeddings, say for example our CNN we previously define.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pert_solver</span></code> : The unconstrained perturbed solver.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pert_csolver</span></code> : The constrained perturbed solver.</p></li>
</ul>
<p>The forward pass calculates the Partial Fenchel-Young loss:</p>
<p><span class="math notranslate nohighlight">\(\ell(\Sigma, M_\Omega) = F_{k,\epsilon}^*(\Sigma) - F_{k,\epsilon}^*(\Sigma, M_\Omega)\)</span></p>
<p>whose gradient will be <span class="math notranslate nohighlight">\(\nabla_\Sigma \ell = A_{k, \epsilon}^*(\Sigma) - A_{k, \epsilon}^*(\Sigma, M_\Omega)\)</span>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class DC(nn.Module):
    &#39;&#39;&#39;
    supervised differentiable clustering
    &#39;&#39;&#39;
    backbone : nn.Module # model backbone used to create embeddings
    pert_solver : Callable # perturbed clustering
    pert_csolver : Callable # perturbed constrained clustering

    # call is equivalent to embedding the data
    @nn.compact
    def __call__(self, *args, **kwargs):
        return self.backbone(*args, **kwargs)

    def similarity(self, Z):
        S =  -pairwise_square_distance(Z)
        # standardizing reduces dependence of sigma on x
        S = (S - S.mean()) / S.std()
        return S

    def forward(self, x, yhot, ncc, key, training=True):
        Z = self.__call__(x, training=training)
        S = self.similarity(Z)
        M_target = yhot @ yhot.T
        C = 2 * M_target - 1

        Ak, Fk, Mk = self.pert_solver(S, ncc, key)
        Akc, Fkc, Mkc = self.pert_csolver(S, ncc, C, key)

        partial_fy_loss = Fk - Fkc
        return partial_fy_loss
<br/></pre></div>
</div>
</div>
<p>In line with <code class="docutils literal notranslate"><span class="pre">flax</span></code>, we will define a train state which encapsulates all parameters (model and optimizer) and methods required for training / evaluation:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class DCTrainState(struct.PyTreeNode):
    step: int
    apply_fn: Callable = struct.field(pytree_node=False)
    forward_fn: Callable = struct.field(pytree_node=False)
    params: core.FrozenDict[str, Any] = struct.field(pytree_node=True)
    tx: optax.GradientTransformation = struct.field(pytree_node=False)
    opt_state: optax.OptState = struct.field(pytree_node=True)

    def apply_gradients(self, *, grads, **kwargs):
        updates, new_opt_state = self.tx.update(
            grads, self.opt_state, self.params)
        new_params = optax.apply_updates(self.params, updates)
        return self.replace(
            step=self.step + 1,
            params=new_params,
            opt_state=new_opt_state,
            **kwargs,
        )

    @classmethod
    def create(cls, *, apply_fn, forward_fn, params, tx, **kwargs):
        opt_state = tx.init(params)
        return cls(
            step=0,
            apply_fn=apply_fn,
            forward_fn=partial(apply_fn, method=forward_fn),
            params=params,
            tx=tx,
            opt_state=opt_state,
            **kwargs,
        )
</pre></div>
</div>
</div>
<p>Let us instantiate our model and optimizer:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>optimizer = optax.adamw(3e-4, weight_decay=1e-4)

model = DC(
    CNN(),
    pert_solver=pert_solver,
    pert_csolver=pert_csolver)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>dummy_x = jnp.ones((BS, ) + (DSHAPE))
params = model.init({&#39;params&#39; : jax.random.PRNGKey(0)}, dummy_x, training=True)[&#39;params&#39;]
<br/></pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from clu import parameter_overview
print(parameter_overview.get_parameter_overview(params))
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
+-------------------------+----------------+---------+-----------+--------+
| Name                    | Shape          | Size    | Mean      | Std    |
+-------------------------+----------------+---------+-----------+--------+
| backbone/Conv_0/bias    | (32,)          | 32      | 0.0       | 0.0    |
| backbone/Conv_0/kernel  | (3, 3, 1, 32)  | 288     | 0.0223    | 0.342  |
| backbone/Conv_1/bias    | (64,)          | 64      | 0.0       | 0.0    |
| backbone/Conv_1/kernel  | (3, 3, 32, 64) | 18,432  | -0.000226 | 0.0587 |
| backbone/Dense_0/bias   | (256,)         | 256     | 0.0       | 0.0    |
| backbone/Dense_0/kernel | (3136, 256)    | 802,816 | -1.17e-05 | 0.0178 |
| backbone/Dense_1/bias   | (256,)         | 256     | 0.0       | 0.0    |
| backbone/Dense_1/kernel | (256, 256)     | 65,536  | -9.22e-05 | 0.0625 |
+-------------------------+----------------+---------+-----------+--------+
Total: 887,680
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>state = DCTrainState.create(
    apply_fn=model.apply,
    forward_fn=model.forward,
    params = params,
    tx = optimizer
)
</pre></div>
</div>
</div>
<p>We can now make a function that performs a single training step given a batch of data. This is <code class="docutils literal notranslate"><span class="pre">jax.jit</span></code> compatible.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>@jax.jit
def train_step_fn(state, X, Yhot, ncc, rngs):
    def forward(params, X, Yhot, ncc, rngs):
        return state.forward_fn({&#39;params&#39; : params}, X, Yhot, ncc, rngs[&#39;noise&#39;], True, rngs=rngs)
    loss, grads = jax.value_and_grad(forward)(state.params, X, Yhot, ncc, rngs)
    state = state.apply_gradients(grads=grads)
    return state, loss, grads
</pre></div>
</div>
</div>
<p>We can use the above function to perform a training loop in just a few lines of code:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>NSTEPS = 300
rngs = {&#39;noise&#39; : jax.random.PRNGKey(0)}

losses = []
for i in tqdm.tqdm(range(NSTEPS)):
      rngs = {k : jax.random.fold_in(v, i) for (k, v) in rngs.items()}
      (X, Yhot), train_iterator = next_train(train_iterator)
      state, pl, grads = train_step_fn(state, X, Yhot, 10, rngs)
      losses.append(pl.item())
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|██████████| 300/300 [00:49&lt;00:00,  6.03it/s]
</pre></div></div>
</div>
<p>Plotting the partial Fenchel-Young loss throughout training with and without smoothing:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt
def moving_average(x, w):
    return np.convolve(x, np.ones(w), &#39;valid&#39;) / w

plt.plot(moving_average(losses, 10), color=&#39;g&#39;, label=&#39;moving average&#39;)
plt.plot(losses, color=&#39;b&#39;, alpha=0.2, label=&#39;raw&#39;)
plt.xlabel(&#39;step&#39;)
plt.ylabel(&#39;Partial FY Loss&#39;)
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Text(0, 0.5, &#39;Partial FY Loss&#39;)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_cnn-mnist_33_1.png" src="../_images/examples_cnn-mnist_33_1.png" />
</div>
</div>
<p>Perform a tSNE visualization of the model embeddings for some of the dataset:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>NTSNE = 3000
X, Y = tfds.load(name=&#39;mnist&#39;, batch_size=-1, data_dir=DATA_DIR, split=&#39;train&#39;, as_supervised=True)
X = np.array(X)
Y = np.array(Y)

X = X[:NTSNE].reshape(-1, 28, 28, 1) / 255.0
Y = Y[:NTSNE].astype(&#39;int&#39;)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from sklearn.manifold import TSNE
V = state.apply_fn({&#39;params&#39; : state.params}, X)
tsne = TSNE(n_components=2).fit_transform(V)
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/Users/lawrencestewart/miniconda3/envs/m1/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:795: FutureWarning: The default initialization in TSNE will change from &#39;random&#39; to &#39;pca&#39; in 1.2.
  warnings.warn(
/Users/lawrencestewart/miniconda3/envs/m1/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:805: FutureWarning: The default learning rate in TSNE will change from 200.0 to &#39;auto&#39; in 1.2.
  warnings.warn(
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>TSNE_COLORS = [
    &#39;#a6cee3&#39;,&#39;#1f78b4&#39;,&#39;#b2df8a&#39;,&#39;#33a02c&#39;,&#39;#fb9a99&#39;,&#39;#e31a1c&#39;,&#39;#fdbf6f&#39;,
    &#39;#ff7f00&#39;,&#39;#cab2d6&#39;,&#39;#6a3d9a&#39;,
]

color_map = np.array([TSNE_COLORS[i] for i in range(10)])

plt.scatter(tsne[:, 0], tsne[:, 1], color=color_map[Y], marker=&#39;.&#39;, alpha=0.4)
plt.title(f&#39;tSNE of {NTSNE} MNIST data embeddings&#39;)
plt.xticks([])
plt.yticks([])
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
([], [])
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_cnn-mnist_37_1.png" src="../_images/examples_cnn-mnist_37_1.png" />
</div>
</div>
<p>The differentiable clustering methodology comes from the following paper:</p>
<p><strong>[Stewart et al. 2023]</strong> - Differentiable Clustering with Perturbed Random Forests, <em>Advances in Neural Information Processing Systems 2023.</em></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../api.html" class="btn btn-neutral float-left" title="API reference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Lawrence Stewart.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>